{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\nimport pandas as pd\nfrom datasets import Dataset, load_dataset, load_from_disk, concatenate_datasets\nimport os\nimport threading\nimport requests\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\nimport shutil\n\nfrom PIL import PngImagePlugin\nLARGE_ENOUGH_NUMBER = 100\nPngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"# os.makedirs(\"/kaggle/working/Google_Conceptual_Caption_Dataset_Images_Captions\", exist_ok=True)\n\n# source_path = \"/kaggle/input/image-retrieval-clip-training-conceptual-caption\"  # Replace with the actual source file path\n# destination_path = \"/kaggle/working/Google_Conceptual_Caption_Dataset_Images_Captions\"  # Replace with the actual destination folder path\n\n# # filenames = [\"/Google_Conceptual_Caption_Dataset_Images_Captions/dataset.arrow\", \"/Google_Conceptual_Caption_Dataset_Images_Captions/dataset_info.json\", \"/Google_Conceptual_Caption_Dataset_Images_Captions/state.json\"]\n\n# import os; filenames = os.listdir(source_path+\"/Google_Conceptual_Caption_Dataset_Images_Captions\")\n\n# filenames = [os.path.join(source_path, \"Google_Conceptual_Caption_Dataset_Images_Captions/\"+file) for file in filenames]\n\n# for file in filenames:\n#     # Copy each file from source to destination\n#     shutil.copy(file, destination_path)\n    \n# dataset = load_from_disk('/kaggle/working/Google_Conceptual_Caption_Dataset_Images_Captions')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_processors = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, load_dataset\n\ndataset = load_dataset(\"shirsh10mall/Image_Captioning_Dataset\", streaming=False ) # split=\"train\"\ndataset","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# yo_ignore = Image.fromarray(np.zeros((8,8,3),dtype=np.uint8))\ndataset = dataset.filter(lambda instance, index: instance[\"drop_this_row\"] == \"no\" and (index<7000 or index>8000), with_indices=True, num_proc=num_processors)\ndataset = dataset.remove_columns([\"drop_this_row\",\"image_url\"])\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Data","metadata":{}},{"cell_type":"code","source":"# for index in range(5,10):\n#     # Display the image using matplotlib\n#     image_array = dataset[\"train\"][index][\"image_data\"]\n#     caption = dataset[\"train\"][index][\"caption\"]\n#     plt.imshow(image_array)\n#     plt.axis('off')  # Turn off axes\n#     print(\"\\n\\n\\n Caption: \", caption )\n#     plt.show()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import CLIPProcessor, CLIPModel, CLIPImageProcessor\nimport torchvision.transforms as transforms\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # Set to \"false\" if needed\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load pre-trained CLIP model and tokenizer\nmodel_name = \"openai/clip-vit-base-patch16\" \ntokenizer = CLIPProcessor.from_pretrained(model_name)\nprocessor = CLIPImageProcessor.from_pretrained(model_name)\n\nmax_length = 22\n\nmodel = CLIPModel.from_pretrained(\"shirsh10mall/Fine_Tuned_CLIP_Model\") # \nmodel.to(device)\nmodel.eval()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define your dataset class\n# class ImageCaptionDataset(Dataset):\n#     def __init__(self, dataset, tokenizer, max_length=64):\n#         self.dataset = dataset\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n#         self.transform = transforms.Compose([   # transforms.Resize(desired_size),  # Resize the PIL image\n#                                                 transforms.ToTensor()  # Convert the PIL image back to tensor\n#                                             ])\n#     def __len__(self):\n#         return len(self.dataset)\n\n#     def __getitem__(self, idx):\n#         instance = self.dataset[idx]\n#         image = self.transform( instance[\"image_data\"] )\n#         if image.shape[0]==1:\n#             image = torch.stack( [image]*3 , dim=1)\n#         elif image.shape[0]==4:\n#             image = image[:3, :, :]\n            \n#         image = processor(images=image, return_tensors=\"pt\").pixel_values\n        \n#         caption = instance[\"caption\"]\n                \n#         text_inputs = self.tokenizer(caption, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        \n#         return { 'pixel_values': image.squeeze(), 'input_ids': text_inputs['input_ids'].squeeze(),\n#                     'attention_mask': text_inputs['attention_mask'].squeeze() }\n    \n\n# train_dataset = ImageCaptionDataset( dataset, tokenizer, max_length )\n# train_dataloader = DataLoader( train_dataset, batch_size=1, shuffle=False , prefetch_factor=2, num_workers=2 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([ transforms.Resize((224,224)), transforms.ToTensor() ])\n\ndef create_image_embeddings(instance):\n    image = transform( instance[\"image_data\"] )\n    if image.shape[0]==1:\n            image = torch.stack( [image]*3 , dim=1)\n            image = image.squeeze()\n            \n    elif image.shape[0]==4:\n        image = image[:3, :, :]\n\n    # image = processor(images=image, return_tensors=\"pt\").pixel_values\n    text_inputs = tokenizer(instance[\"caption\"], padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n\n    with torch.no_grad():\n        outputs = model( pixel_values=image.unsqueeze(0).to(device) , input_ids = text_inputs['input_ids'].to(device),\n                            attention_mask=text_inputs['attention_mask'].to(device) )\n    \n\n    instance[\"image_embeddings\"] =  outputs.image_embeds.cpu().numpy()[0]\n    instance[\"text_embeddings\"] = outputs.text_embeds.cpu().numpy()[0]\n\n    return instance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset = dataset.add_column(\"image_embeddings\", image_embeddings_list)\n\ndataset = dataset.map( create_image_embeddings, num_proc=num_processors ) # num_processors","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm import tqdm \n\n# image_embeddings_list = [0]*len(train_dataloader)\n\n# model.to(device)\n# batch_idx = 0\n# for batch in tqdm(train_dataloader):\n#     pixel_values = batch['pixel_values'].to(device)\n#     input_ids = batch['input_ids'].to(device)\n#     attention_mask = batch['attention_mask'].to(device)\n#     with torch.no_grad():\n#         outputs = model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask)\n#         image_embeddings_list[batch_idx] = outputs.image_embeds.cpu().numpy()[0]\n#     batch_idx=batch_idx+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n\n# def get_list_memory_usage(my_list):\n#     size_bytes = sys.getsizeof(my_list)\n#     size_mb = size_bytes / (1024 ** 2)\n#     return size_bytes, size_mb\n\n# bytes_used, mb_used = get_list_memory_usage(image_embeddings_list)\n# print(f\"List occupies {bytes_used} bytes or approximately {mb_used:.4f} MB\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.save_to_disk(\"/kaggle/working/Image_Captioning_GCC_Embeddings\")\ndataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom huggingface_hub import login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"SHIRSH_HUGGINGFACE_API_KEY\")\n\nlogin(token=secret_value_0, write_permission=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    dataset.push_to_hub(\"shirsh10mall/Image_Captioning_GCC_Embeddings\")\nexcept:\n    print(\"skipped\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset.save_to_disk(\"/kaggle/working/Google_Conceptual_Caption_Dataset_Images_Embeddings_Captions\")\n# dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def calculate_similarity_scores(query_encoding,image_embeddings):\n#     similarity_score = torch.cosine_similarity(query_encoding, torch.tensor(image_embeddings).to(device), dim=-1).item()\n#     return similarity_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Function to perform image retrieval\n\n# max_length = 32\n\n# def image_retrieval(query, dataset, counter_n=3, display=False, search_type=\"normal\"):\n#     # Encode the query text\n#     query_input = tokenizer(query, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n#     query_input.to(device)\n#     query_encoding = model.get_text_features(**query_input)\n    \n#     if search_type==\"normal\":\n#         # Calculate similarity scores between query and images\n#         dataset = dataset.map( lambda instance: { \"similarity_scores\": calculate_similarity_scores(query_encoding,instance[\"image_embeddings\"]) } )\n\n#         # Sort images by similarity scores\n#         counter_n = 0\n#         sorted_images = []\n#         for index in torch.argsort(torch.tensor(dataset[\"similarity_scores\"]), descending=True):\n#             counter_n = counter_n + 1\n#             sorted_images.append( dataset[int(index)][\"image_data\"] )\n#             if counter_n>=3:\n#                 break\n#         dataset = dataset.remove_columns([\"similarity_scores\"])\n#     # elif search_type==\"faiss\":\n    \n#     if display:\n#         for i, image in enumerate(sorted_images):\n#             plt.imshow(image)\n#             plt.axis('off')  # Turn off axes\n#             print(\"\\n\\n\\n\" )\n#             plt.show()\n    \n#     return sorted_images, dataset  # Return top 3 similar images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # User input query\n# query = \"dogs\"\n\n# # Perform image retrieval\n# retrieved_images, dataset = image_retrieval(query, dataset, display=True, counter_n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # User input query\n# query = \"game\"\n\n# # Perform image retrieval\n# retrieved_images, dataset = image_retrieval(query, dataset, display=True, counter_n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # User input query\n# query = \"sport\"\n\n# # Perform image retrieval\n# retrieved_images, dataset = image_retrieval(query, dataset, display=True, counter_n=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install faiss-cpu\n# !pip install faiss-gpu\n\n# dataset.add_faiss_index(column=\"image_embeddings\")\n# dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}