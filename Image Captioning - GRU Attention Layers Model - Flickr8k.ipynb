{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning","metadata":{"papermill":{"duration":0.007587,"end_time":"2023-06-28T14:56:13.026950","exception":false,"start_time":"2023-06-28T14:56:13.019363","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\n!pip install tqdm\nfrom tqdm import tqdm\ntqdm.pandas()\nimport numpy as np\nimport re\n# text\nimport string\nfrom PIL import Image\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","metadata":{"_kg_hide-input":true,"papermill":{"duration":47.81566,"end_time":"2023-06-28T14:57:00.849282","exception":false,"start_time":"2023-06-28T14:56:13.033622","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # connect to a TPU and instantiate a distribution strategy\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.TPUStrategy(tpu)","metadata":{"_kg_hide-input":true,"papermill":{"duration":8.574255,"end_time":"2023-06-28T14:57:09.431158","exception":false,"start_time":"2023-06-28T14:57:00.856903","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read text data from txt file using pandas\ntext_data = pd.read_csv('/kaggle/input/flickr8k/captions.txt', sep='|')\n# text_data.columns = ['image_id', 'caption']\ntext_data","metadata":{"papermill":{"duration":0.136752,"end_time":"2023-06-28T14:57:09.577675","exception":false,"start_time":"2023-06-28T14:57:09.440923","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the image name from the image id\n# text_data['image_name'] = text_data['image_id'].apply(lambda x: x.split('#')[0])\n# text_data['image_repeat'] = text_data['image_id'].apply(lambda x: x.split('#')[1])\n\ntext_data.rename(columns={\"caption_number\":\"image_repeat\",\"caption_text\":\"caption\"},inplace=True)\ntext_data","metadata":{"papermill":{"duration":0.029965,"end_time":"2023-06-28T14:57:09.618352","exception":false,"start_time":"2023-06-28T14:57:09.588387","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows with given index\ntext_data.drop(text_data[text_data['image_name']==\"2258277193_586949ec62.jpg.1\"].index, inplace=True)\ntext_data.reset_index(drop=True, inplace=True)","metadata":{"papermill":{"duration":0.024061,"end_time":"2023-06-28T14:57:09.652450","exception":false,"start_time":"2023-06-28T14:57:09.628389","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing_text( line ):\n    # lowercase all the english words\n    line  = line.lower()\n    # remove punctuation\n    line = line.translate(str.maketrans('', '', string.punctuation))\n    return line\n\nprint(\"Preprocessing Text\")\ntext_data['caption'] = text_data['caption'].progress_apply( preprocessing_text )\n# data.dropna( inplace=True )\n# data.reset_index(drop=True, inplace=True)\ntext_data","metadata":{"papermill":{"duration":0.307815,"end_time":"2023-06-28T14:57:09.969971","exception":false,"start_time":"2023-06-28T14:57:09.662156","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add start and end tokens into english sentences\ntext_data['caption'] = text_data['caption'].progress_apply( lambda x: 'START '+ x + ' END' )\n\n# Tokenize the english sentences using Keras tokenizer\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer( lower=False , split=' ', char_level=False, oov_token=\"oovE\",filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') # num_words=num_words\ntokenizer.fit_on_texts(text_data['caption'].values)\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary Size: \", vocab_size)\n\n# Convert the english sentences into sequences\nsequences = tokenizer.texts_to_sequences(text_data['caption'].values)\nmax_length = max([len(line) for line in sequences])\n\n# padding sequence (max_length)\npadded_sequences = []\nfor sequence in tqdm(sequences):\n    if len(sequence) < max_length:\n        sequence += [0] * (max_length - len(sequence))\n    padded_sequences.append(sequence)\n\n# print(tokenizer.word_index)\nprint(text_data['caption'][0])\nprint(sequences[0])\nprint(padded_sequences[0])","metadata":{"papermill":{"duration":1.709502,"end_time":"2023-06-28T14:57:11.689863","exception":false,"start_time":"2023-06-28T14:57:09.980361","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the padding from the sequence\ndef remove_padding_from_sequence(sequence):\n    return [word for word in sequence if word != 0]\n    \n# Print the sequence and compare with the original sentence using keras\nn = 2\nfor j in range( n ):\n    i = np.random.randint( text_data.shape[0] )\n    print( i )\n    print(\"Original Sentence ---> \", text_data[\"caption\"][i])\n    print(\"Sequence ---> \", padded_sequences[i])\n    print(\" Sequence into Sentence  ---> \", tokenizer.sequences_to_texts( [remove_padding_from_sequence(padded_sequences[i])] )) \n    print(\"---------------------   -  --------------------- \\n \")\n","metadata":{"papermill":{"duration":0.021958,"end_time":"2023-06-28T14:57:11.722866","exception":false,"start_time":"2023-06-28T14:57:11.700908","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data['padded_sequences'] = padded_sequences\ntext_data","metadata":{"papermill":{"duration":0.039433,"end_time":"2023-06-28T14:57:11.773318","exception":false,"start_time":"2023-06-28T14:57:11.733885","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop rows with given value of column\ntext_data_train = text_data.drop(text_data[text_data['image_repeat']==0].index)\ntext_data_train.reset_index(drop=True, inplace=True)\ntext_data_val = text_data[ text_data['image_repeat']==0 ]\ntext_data_val.reset_index(drop=True, inplace=True)\ntext_data_train.shape, text_data_val.shape","metadata":{"papermill":{"duration":0.032484,"end_time":"2023-06-28T14:57:11.817277","exception":false,"start_time":"2023-06-28T14:57:11.784793","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create images dataset from file names\nimage_width = 224\nimage_height = 224\n\ndef load_image(image_path, filenames):\n    image_data = np.empty((len(filenames), image_height, image_width, 3),dtype=np.uint8)\n    for i in tqdm(range(len(filenames))):\n        image_data[i] = np.array(Image.open(image_path + filenames[i]).resize((image_width, image_height)), dtype=np.uint8)\n    # image_data = tf.convert_to_tensor(image_data, dtype=tf.int32)\n    return image_data\n    \n\nimage_data_train = load_image('/kaggle/input/flickr8k/images/', text_data_train['image_name'])\nimage_data_val = load_image('/kaggle/input/flickr8k/images/', text_data_val['image_name'])\n\nimage_data_train.shape, image_data_val.shape","metadata":{"papermill":{"duration":393.279595,"end_time":"2023-06-28T15:03:45.108454","exception":false,"start_time":"2023-06-28T14:57:11.828859","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display fews images\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(image_data_train[i])\n    plt.axis('off')\n    # caption (small font size)\n    plt.text(0, 2, text_data_train['caption'][i], fontsize=12)\n    # largen plot size\n    plt.gcf().set_size_inches(40, 30)\n    \nplt.show()","metadata":{"papermill":{"duration":2.955888,"end_time":"2023-06-28T15:03:48.378909","exception":false,"start_time":"2023-06-28T15:03:45.423021","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_data_train_in = np.concatenate( (np.array(list(text_data_train['padded_sequences']))[:, :-1] , np.zeros((image_data_train.shape[0],1), dtype=np.int32) ), axis=1 )\ntext_data_train_out = np.concatenate( (np.array(list(text_data_train['padded_sequences']))[:, 1:] , np.zeros((image_data_train.shape[0],1), dtype=np.int32) ), axis=1 )\ntext_data_test_in = np.concatenate( (np.array(list(text_data_val['padded_sequences']))[:, :-1] , np.zeros((image_data_val.shape[0],1), dtype=np.int32)) , axis=1 )\ntext_data_test_out = np.concatenate( ( np.array(list(text_data_val['padded_sequences']))[:, 1:] , np.zeros((image_data_val.shape[0],1), dtype=np.int32)) , axis=1 )\n\nimage_data_train.shape, image_data_val.shape, text_data_train_in.shape, text_data_train_out.shape, text_data_test_in.shape, text_data_test_out.shape","metadata":{"papermill":{"duration":0.663323,"end_time":"2023-06-28T15:03:49.397913","exception":false,"start_time":"2023-06-28T15:03:48.734590","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{"papermill":{"duration":0.346525,"end_time":"2023-06-28T15:03:50.129210","exception":false,"start_time":"2023-06-28T15:03:49.782685","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, InputLayer, Embedding, LSTM, GRU, TimeDistributed, RepeatVector, Dense, Bidirectional, Flatten, LayerNormalization, Add, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.applications.resnet50 import ResNet50","metadata":{"papermill":{"duration":0.346937,"end_time":"2023-06-28T15:03:50.813972","exception":false,"start_time":"2023-06-28T15:03:50.467035","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size =  8 * tpu_strategy.num_replicas_in_sync # 16\nepochs = 150\n# steps_per_execution = 32","metadata":{"papermill":{"duration":0.343474,"end_time":"2023-06-28T15:03:51.492248","exception":false,"start_time":"2023-06-28T15:03:51.148774","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loss Function # Masked Sparse Categorical Cross Entropy\ndef masked_categorical_cross_entropy( y_true, y_pred ):\n    # Calculate the loss for each item in the batch.\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n    # Mask off the losses on padding.\n    mask = tf.cast(y_true != 0, loss.dtype)\n    loss *= mask\n    # Return the total.\n    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n\n# Accuracy Metric\ndef masked_accuracy(y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    y_pred = tf.argmax(y_pred, axis=-1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n    match = tf.cast(y_true == y_pred, tf.float32)\n    mask = tf.cast(y_true != 0, tf.float32)\n    return tf.reduce_sum(match)/tf.reduce_sum(mask)","metadata":{"papermill":{"duration":0.347821,"end_time":"2023-06-28T15:03:52.204769","exception":false,"start_time":"2023-06-28T15:03:51.856948","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Creating the model\n# channels = 3\n# units = 2048\n\n# # instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\n\n#     ######  Image Encoder\n#     class Image_Encoder(tf.keras.layers.Layer):\n#         def __init__( self, input_shape, units, image_model=MobileNetV2, trainable=True ):\n#             super(Image_Encoder,self).__init__()\n#             self.input_image_shape = input_shape\n#             self.image_model = image_model(include_top=False, weights='imagenet', input_shape=self.input_image_shape)\n#             self.trainable = trainable\n#             self.units = units\n#             self.dense = Dense(units, activation='relu')\n\n#         def call( self, image_encoder_inputs ):\n#             image_features = self.image_model(image_encoder_inputs)\n#             # Flatten\n#             image_features = tf.keras.layers.Reshape((image_features.shape[1]*image_features.shape[2], image_features.shape[3]))( image_features )\n#             # Dense   # ********** #\n#             image_features = self.dense(image_features) # ********** #\n#             return image_features\n\n#         def get_config(self):\n#             config = super(Image_Encoder,self).get_config()\n#             config.update({ 'input_image_shape': self.input_image_shape, 'image_model': self.image_model, 'units': self.units, 'dense': self.dense, \"trainable\":self.trainable })\n#             return config\n\n#     class Decoder(tf.keras.layers.Layer):\n#         def __init__(self, vocab_size, units, max_length):\n#             super(Decoder,self).__init__()\n#             self.vocab_size = vocab_size\n#             self.units = units\n#             self.max_length = max_length\n#             self.Embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=self.units, input_length=self.max_length , mask_zero=True)\n#             self.gru_layer = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True)\n#             self.attention_layer = tf.keras.layers.Attention()\n#             self.add = Add()\n#             self.layernorm = LayerNormalization(axis=-1)\n#             self.dense = tf.keras.layers.Dense(vocab_size)\n\n#         def call( self, text_encoder_inputs, image_features ):\n#             # Embedding\n#             length = self.max_length\n#             encoder = self.Embedding\n#             encoder_embedding = encoder(text_encoder_inputs)\n#             # RNN\n#             gru_output, gru_state = self.gru_layer(encoder_embedding)\n#             # Attention\n#             # key & query --> gru_output | value --> encoder_output\n#             context_vector = self.attention_layer([gru_output, image_features])\n#             addition = self.add([gru_output,context_vector])\n#             norm_layer = self.layernorm(addition)\n#             # Dense\n#             output = self.dense(norm_layer) # overall_decoder_attention_output\n#             return output, gru_state\n\n#         def get_config(self):\n#             config = super(Decoder,self).get_config()\n#             config.update({ 'vocab_size': self.vocab_size, 'units': self.units, 'max_length': self.max_length, 'Embedding': self.Embedding, \"gru_layer\":self.gru_layer,\n#                              'attention_layer':self.attention_layer, 'add':self.add, 'layernorm':self.layernorm, \"dense\":self.dense })\n#             return config\n\n\n#     # Inputs\n#     text_encoder_inputs = Input(shape=(max_length,))\n#     image_encoder_inputs = Input(shape=(image_height, image_width, channels))\n\n#     # Image Encoding\n#     image_encoder = Image_Encoder( input_shape=(image_height, image_width, channels), image_model=ResNet50, units=units, trainable=True )\n#     image_features = image_encoder( image_encoder_inputs )\n\n#     # Decoding Caption\n#     decoder = Decoder(vocab_size=vocab_size, units=units, max_length=max_length)\n#     output, gru_state = decoder( text_encoder_inputs, image_features )\n\n#     # Model\n#     training_model = tf.keras.Model(inputs=[image_encoder_inputs,text_encoder_inputs], outputs=output)   \n    \n#     # Mark all layers in the model as trainable\n#     for layer in training_model.layers:\n#         layer.trainable = True\n\n#     # Compile\n#     lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay( initial_learning_rate=1e-4, decay_steps=1000)\n#     training_model.compile(loss=masked_categorical_cross_entropy, optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_decayed_fn),metrics=[masked_accuracy]) # Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)\n#     training_model.summary()","metadata":{"papermill":{"duration":44.680843,"end_time":"2023-06-28T15:04:37.220288","exception":false,"start_time":"2023-06-28T15:03:52.539445","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the model\nchannels = 3\nunits = 1024\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n\n    # Image Encoder\n    class Image_Encoder(tf.keras.layers.Layer):\n        def __init__(self, input_shape, units, image_model=MobileNetV2, trainable=True):\n            super(Image_Encoder, self).__init__()\n            self.input_image_shape = input_shape\n            self.image_model = image_model(include_top=False, weights='imagenet', input_shape=self.input_image_shape)\n            self.trainable = trainable\n            self.units = units\n            self.dense = Dense(units, activation='relu')\n\n        def call(self, image_encoder_inputs):\n            image_features = self.image_model(image_encoder_inputs)\n            # Flatten\n            image_features = tf.keras.layers.Reshape((image_features.shape[1] * image_features.shape[2], image_features.shape[3]))(image_features)\n            # Dense\n            image_features = self.dense(image_features)\n            return image_features\n\n        def get_config(self):\n            config = super(Image_Encoder, self).get_config()\n            config.update({'input_image_shape': self.input_image_shape, 'image_model': self.image_model, 'units': self.units, 'dense': self.dense, \"trainable\": self.trainable})\n            return config\n\n    class Decoder(tf.keras.layers.Layer):\n        def __init__(self, vocab_size, units, max_length, num_gru_layers=2):\n            super(Decoder, self).__init__()\n            self.vocab_size = vocab_size\n            self.units = units\n            self.max_length = max_length\n            self.num_gru_layers = num_gru_layers\n            self.Embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=self.units, input_length=self.max_length, mask_zero=True)\n            \n            # List of GRU layers\n            self.gru_layers = [tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True) for _ in range(self.num_gru_layers)]\n            \n            self.attention_layer = tf.keras.layers.Attention()\n            self.add = Add()\n            self.layernorm = LayerNormalization(axis=-1)\n            self.dense = tf.keras.layers.Dense(vocab_size)\n\n            # Skip connection layers\n            self.image_feature_projection = tf.keras.layers.Dense(units)\n            self.image_feature_embedding = tf.keras.layers.Dense(units)\n\n        def call(self, text_encoder_inputs, image_features):\n            # Embedding\n            length = self.max_length\n            encoder = self.Embedding\n            encoder_embedding = encoder(text_encoder_inputs)\n\n            # Pass through GRU layers\n            gru_output = encoder_embedding\n            for gru_layer in self.gru_layers:\n                gru_output, gru_state = gru_layer(gru_output)\n\n            # Attention\n            # key & query --> gru_output | value --> encoder_output\n            context_vector = self.attention_layer([gru_output, image_features])\n            addition = self.add([gru_output, context_vector])\n            norm_layer = self.layernorm(addition)\n            \n            # Dense\n            output = self.dense(norm_layer)  # overall_decoder_attention_output\n\n            # Skip connections\n            image_projection = self.image_feature_projection(image_features)\n            image_embedding = self.image_feature_embedding(image_projection)\n            \n            # Adjust dimensions for skip connection\n            image_embedding = tf.reduce_mean(image_embedding, axis=1, keepdims=True)  # Pooling across time steps\n            image_embedding = tf.tile(image_embedding, [1, tf.shape(gru_output)[1], 1])  # Tile to match time steps\n            \n            image_skip_connection = self.add([gru_output, image_embedding])\n            output = tf.concat([output, image_skip_connection], axis=-1)\n\n            return output, gru_state\n\n        def get_config(self):\n            config = super(Decoder, self).get_config()\n            config.update({'vocab_size': self.vocab_size, 'units': self.units, 'max_length': self.max_length, 'Embedding': self.Embedding, \"gru_layers\": self.gru_layers,\n                           'attention_layer': self.attention_layer, 'add': self.add, 'layernorm': self.layernorm, \"dense\": self.dense,\n                           'image_feature_projection': self.image_feature_projection, 'image_feature_embedding': self.image_feature_embedding})\n            return config\n\n    # Inputs\n    text_encoder_inputs = Input(shape=(max_length,))\n    image_encoder_inputs = Input(shape=(image_height, image_width, channels))\n\n    # Image Encoding\n    image_encoder = Image_Encoder(input_shape=(image_height, image_width, channels), image_model=ResNet50, units=units, trainable=True)\n    image_features = image_encoder(image_encoder_inputs)\n\n    # Decoding Caption\n    decoder = Decoder(vocab_size=vocab_size, units=units, max_length=max_length, num_gru_layers=5)  # Added 3 GRU layers\n    output, gru_state = decoder(text_encoder_inputs, image_features)\n\n    # Model\n    training_model = tf.keras.Model(inputs=[image_encoder_inputs, text_encoder_inputs], outputs=output)\n\n    # Mark all layers in the model as trainable\n    for layer in training_model.layers:\n        layer.trainable = True\n\n    # Compile\n    # lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=1e-4, decay_steps=1000)\n    training_model.compile(loss=masked_categorical_cross_entropy, optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-4), metrics=[masked_accuracy])\n    training_model.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# early_stopping = EarlyStopping(monitor='loss', patience=20)\ncheckpoint = ModelCheckpoint(\"Image_Captioning_Transfomer_Model\", moniter='loss',save_best_only=True, save_weights_only=False, mode='min')\n# reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, patience=2, min_lr=1e-10,mode=\"min\")\nhistory = training_model.fit( [image_data_train,text_data_train_in], text_data_train_out, validation_data=([image_data_val,text_data_test_in],text_data_test_out), \n                                 epochs=epochs, batch_size=batch_size, callbacks=[checkpoint] )","metadata":{"papermill":{"duration":16846.973989,"end_time":"2023-06-28T19:45:24.536545","exception":false,"start_time":"2023-06-28T15:04:37.562556","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup for Inference","metadata":{"papermill":{"duration":9.96722,"end_time":"2023-06-28T19:45:44.467229","exception":false,"start_time":"2023-06-28T19:45:34.500009","status":"completed"},"tags":[]}},{"cell_type":"code","source":"inference_model = tf.keras.Model(inputs=training_model.input, outputs=training_model.layers[3].output)\n# output, gru_state = inference_model.predict( [ image_data_train[0:2], text_data_train_in[0:2]] )","metadata":{"papermill":{"duration":10.003559,"end_time":"2023-06-28T19:46:04.306413","exception":false,"start_time":"2023-06-28T19:45:54.302854","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_caption( image, model ):\n    \n    # Display fews images\n    plt.figure(figsize=(4,4))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n        \n    image = np.expand_dims(image, axis=0) \n    initial_token = tokenizer.texts_to_sequences([\"START\"])[0]\n    initial_token += [0] * (max_length - len(initial_token))\n\n    gru_state = np.zeros( (1,units) )\n    tokens = np.expand_dims( np.array( initial_token ), axis=0 )\n    \n    for i in range(1,max_length):\n        output, gru_state = model.predict( [image,tokens], verbose=0 )\n        current_pred_token = np.argmax(output[:,-1,:],axis=-1)\n\n        tokens[0][i] = current_pred_token\n\n    pred_caption = tokenizer.sequences_to_texts( tokens )\n    print( \"Predicted Caption: \", pred_caption )\n    \n    return pred_caption\n\nindex = 10\nprint( \"Original Caption\", \" | \".join(list(text_data_train[ text_data_train['image_name']==text_data_train['image_name'][index] ]['caption'].values)) )\npred_caption = create_caption( image=image_data_train[index], model=inference_model )","metadata":{"papermill":{"duration":33.41473,"end_time":"2023-06-28T19:46:47.777542","exception":false,"start_time":"2023-06-28T19:46:14.362812","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Caption","metadata":{"papermill":{"duration":9.995262,"end_time":"2023-06-28T19:47:07.684112","exception":false,"start_time":"2023-06-28T19:46:57.688850","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for index in np.random.randint(int(image_data_train.shape[0]/4 - 1),size=20):\n    print( \"Original Caption\", \" | \".join(list(text_data_train[ text_data_train['image_name']==text_data_train['image_name'][index] ]['caption'].values)) )\n    pred_caption = create_caption( image=image_data_train[index], model=inference_model )\n    print( \"---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - ---- - \\n \\n\" )","metadata":{"papermill":{"duration":409.872268,"end_time":"2023-06-28T19:54:07.584228","exception":false,"start_time":"2023-06-28T19:47:17.711960","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***","metadata":{"papermill":{"duration":10.113561,"end_time":"2023-06-28T19:54:27.735190","exception":false,"start_time":"2023-06-28T19:54:17.621629","status":"completed"},"tags":[]}}]}